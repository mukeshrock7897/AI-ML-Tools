{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# ğŸ§­ LangSmith â€” Ultimate Cheatsheet\n",
    "\n",
    "> **Mental model:** LangSmith = **observability + evaluations** for LLM apps. It records **runs/spans/traces**, manages **datasets & experiments**, collects **feedback**, supports **A/B testing**, and exports **OpenTelemetry** to your own stack. Works standalone or with **LangChain/LangGraph**.\n",
    "\n",
    "---\n",
    "\n",
    "## 0) Setup & SDKs âš™ï¸\n",
    "\n",
    "| What                     | How (Python/JS)                                             | Notes                                    |\n",
    "| ------------------------ | ----------------------------------------------------------- | ---------------------------------------- |\n",
    "| Create account & API key | Settings â†’ **API Keys**                                     | Use separate keys per env (dev/stg/prod) |\n",
    "| Enable tracing (JS)      | Env vars or pass `LangChainTracer`                          | Works with JS LangChain out of the box   |\n",
    "| Python SDK               | `pip install -U langsmith` â†’ `from langsmith import Client` | Use `traceable` decorator for quick wins |\n",
    "| Framework-agnostic       | Toggle via env vars or thin wrappers                        | Works with LC/LG, or any custom stack    |\n",
    "\n",
    "**Soundbite:** *â€œAPI key + env var â†’ instant runs/spans across Python & JS.â€*\n",
    "\n",
    "---\n",
    "\n",
    "## 1) Tracing & Observability ğŸ”\n",
    "\n",
    "| Concept             | What to say                                                          | Practical tip                                       |\n",
    "| ------------------- | -------------------------------------------------------------------- | --------------------------------------------------- |\n",
    "| Runs/Spans/Traces   | Every model/tool call = **run** (span); nested runs form a **trace** | Expand run tree to debug latency/tokens/errors      |\n",
    "| Current span access | Log custom breadcrumbs/IDs inside a traced function                  | Attach request IDs, user IDs (hashed), release tags |\n",
    "| Run schema          | IDs, inputs/outputs, timings, type, error fields                     | Keep inputs minimal; mask sensitive data            |\n",
    "| Tags & metadata     | Add `tags` + `metadata` for env, release, feature                    | Make filtering & dashboards effortless              |\n",
    "| Sampling & PII      | Tune sampling; redact before upload                                  | Define redaction policy centrally                   |\n",
    "| OpenTelemetry       | Export to Prometheus/Grafana/Jaeger                                  | Unify LLM metrics with app/infra SLOs               |\n",
    "\n",
    "**Quick Python (enable + tag runs):**\n",
    "\n",
    "```python\n",
    "import os\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = \"lsk_...\"\n",
    "from langsmith import Client, traceable\n",
    "\n",
    "client = Client(\n",
    "    tracing_sampling_rate=0.2,  # 20% sampling\n",
    "    metadata={\"service\":\"chat-api\",\"env\":\"prod\"}\n",
    ")\n",
    "\n",
    "@traceable(tags=[\"prod\",\"chat\"], metadata={\"release\":\"2025.10.1\"})\n",
    "def answer(q: str): ...\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 2) Datasets & Evaluations ğŸ“Š\n",
    "\n",
    "| Task                          | How                                                     | Notes                                          |\n",
    "| ----------------------------- | ------------------------------------------------------- | ---------------------------------------------- |\n",
    "| Create/manage datasets        | UI (Datasets â†’ New) or SDK (`create_examples`)          | Keep dataset immutable; use versions           |\n",
    "| Run evaluations               | `from langsmith.evaluation import evaluate`             | Name experiments clearly (`experiment_prefix`) |\n",
    "| Compare experiments (A/B)     | Run on same dataset; compare metrics in Experiments     | Track deltas: quality, latency, cost           |\n",
    "| Feedback-driven evals         | Log feedback on runs; slice by tag/feature/user         | Map thumbs-up/down to numeric scores           |\n",
    "| Built-in vs custom evaluators | Use built-ins (QA/faithfulness/context) or write custom | Start with built-ins; add domain checks later  |\n",
    "\n",
    "**Python: eval on a dataset**\n",
    "\n",
    "```python\n",
    "from langsmith import Client\n",
    "from langsmith.evaluation import evaluate\n",
    "\n",
    "client = Client()\n",
    "report = evaluate(\n",
    "  target=my_chain,                     # callable or endpoint\n",
    "  data=\"my_rag_dataset\",               # dataset name/id\n",
    "  evaluators=[\"qa\", \"context_precision\"],\n",
    "  experiment_prefix=\"rag-oct16\"\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 3) Feedback & Annotations ğŸ“\n",
    "\n",
    "| Need                   | API / UX                                                        | Notes                                          |\n",
    "| ---------------------- | --------------------------------------------------------------- | ---------------------------------------------- |\n",
    "| User thumbs-up/down    | `Client().create_feedback(run_id, key=\"user-score\", score=1.0)` | Use consistent keys (`\"user-score\"`, `\"csat\"`) |\n",
    "| Frontend-safe feedback | Presigned token for browser submissions                         | Avoid exposing API key; expire tokens          |\n",
    "| Inline annotations     | Annotate a run or queue tasks for reviewers                     | Great for agent trajectories                   |\n",
    "| Feedback schema        | Human, LLM, or system feedback                                  | Store reason text + score for analytics        |\n",
    "\n",
    "**One-liner:** *â€œUser feedback is first-class and correlates with cost/latency over time.â€*\n",
    "\n",
    "---\n",
    "\n",
    "## 4) Prompt Management ğŸ§±\n",
    "\n",
    "| Capability             | How                                           | Notes                                            |\n",
    "| ---------------------- | --------------------------------------------- | ------------------------------------------------ |\n",
    "| Version & push prompts | Programmatically push named prompts with tags | Treat prompts like code: description + changelog |\n",
    "| A/B prompt comparisons | Run variants on same dataset & compare        | Pair with snapshot tests to catch regressions    |\n",
    "\n",
    "---\n",
    "\n",
    "## 5) Integrations ğŸ”Œ\n",
    "\n",
    "| With                 | How                                              | Notes                                    |\n",
    "| -------------------- | ------------------------------------------------ | ---------------------------------------- |\n",
    "| LangChain (Py/JS)    | Env-based tracing or `traceable`; wrap providers | Add tags per feature/route               |\n",
    "| LangGraph            | Use `RunnableConfig` to pass tags/metadata       | Name nodes consistently to map hot paths |\n",
    "| OpenTelemetry stacks | Export traces/metrics to your collector          | Build cost/latency dashboards in Grafana |\n",
    "\n",
    "---\n",
    "\n",
    "## 6) Production Workflows ğŸ­\n",
    "\n",
    "| Theme             | Checklist                                                                    |\n",
    "| ----------------- | ---------------------------------------------------------------------------- |\n",
    "| Debug & replay    | Reproduce with same inputs; diff runs; inspect error paths; attach snapshots |\n",
    "| Canary & rollouts | Compare new vs baseline; gate on quality deltas and p95 latency              |\n",
    "| Monitoring        | SLOs for latency/cost; rate limits/backpressure; OTel export enabled         |\n",
    "\n",
    "---\n",
    "\n",
    "## 7) Security & Privacy ğŸ”’\n",
    "\n",
    "| Concern       | What you do                               | Notes                                  |\n",
    "| ------------- | ----------------------------------------- | -------------------------------------- |\n",
    "| PII in traces | Client-side masking/redaction/anonymizers | Hash emails/IDs; remove secrets        |\n",
    "| Sampling      | Set `tracing_sampling_rate` per env       | Higher in dev, lower in prod (but >0%) |\n",
    "| Data paths    | Use tags/metadata to isolate envs/tenants | Tie to retention policies              |\n",
    "\n",
    "---\n",
    "\n",
    "## 8) CI/CD & Automation ğŸ¤–\n",
    "\n",
    "| Goal                  | How                                           | Notes                                        |\n",
    "| --------------------- | --------------------------------------------- | -------------------------------------------- |\n",
    "| Gate releases on eval | Run `evaluate(...)` in CI; fail on regression | Thresholds per metric (quality/latency/cost) |\n",
    "| Example pipelines     | Use GH Actions or your CI to run eval suites  | Store reports as artifacts; link to runs     |\n",
    "| OTel in prod          | Pipe metrics/traces to Prometheus/Grafana     | Alert on error spikes and p95 surges         |\n",
    "\n",
    "---\n",
    "\n",
    "## 9) â€œAnswer-in-a-Minuteâ€ Snippets âš¡\n",
    "\n",
    "**A) Minimal tracing (Python)**\n",
    "\n",
    "```python\n",
    "from langsmith import traceable\n",
    "@traceable(tags=[\"api\"], metadata={\"endpoint\":\"/chat\"})\n",
    "def chat(q): ...\n",
    "```\n",
    "\n",
    "**B) Attach user feedback**\n",
    "\n",
    "```python\n",
    "from langsmith import Client\n",
    "Client().create_feedback(run_id, key=\"user-score\", score=1.0)\n",
    "```\n",
    "\n",
    "**C) Create dataset & run eval**\n",
    "\n",
    "```python\n",
    "from langsmith import Client\n",
    "from langsmith.evaluation import evaluate\n",
    "c = Client()\n",
    "c.create_examples(dataset_name=\"faq\",\n",
    "                  inputs=[{\"q\":\"hi\"}],\n",
    "                  outputs=[{\"a\":\"hello\"}])\n",
    "report = evaluate(target=my_chain, data=\"faq\", evaluators=[\"qa\"])\n",
    "```\n",
    "\n",
    "**D) OTel export (high level)**\n",
    "\n",
    "* Enable LangSmith OTel â†’ forward to collector â†’ build Grafana dashboards.\n",
    "\n",
    "---\n",
    "\n",
    "## 10) Checklists âœ…\n",
    "\n",
    "**Prod Readiness**\n",
    "\n",
    "* ğŸ”– Tags/metadata (env, release, user) â€¢ ğŸ§° Sampling set â€¢ ğŸ•µï¸ PII redaction on inputs/outputs\n",
    "* ğŸ§ª Dataset + baseline experiment â€¢ ğŸ§¯ Error paths traced â€¢ ğŸ“Š OTel â†’ Grafana/Prometheus\n",
    "* ğŸš¦ CI gates on eval deltas â€¢ ğŸ§¾ Cost/latency dashboards\n",
    "\n",
    "**RAG/Agent Eval**\n",
    "\n",
    "* ğŸ“š Dataset coverage & slices â€¢ ğŸ§ª Faithfulness/context precision â€¢ ğŸ§­ Agent step/trajectory checks\n",
    "* ğŸ§‰ Human feedback pipeline (inline/queue) â€¢ ğŸ” Compare experiments before rollout\n",
    "\n",
    "---\n",
    "\n",
    "## 11) Common Pitfalls ğŸš« â†’ Fix âœ…\n",
    "\n",
    "| Pitfall                | Fix                                                     |\n",
    "| ---------------------- | ------------------------------------------------------- |\n",
    "| No evaluators/contract | Use built-in evaluators + snapshot tests                |\n",
    "| Missing tags/metadata  | Standardize `env`, `release`, `feature`, `user_id_hash` |\n",
    "| Logging raw PII        | Apply redaction masks before tracing                    |\n",
    "| 0% sampling in prod    | Set a small but nonzero sampling rate (e.g., 1â€“5%)      |\n",
    "\n",
    "---\n",
    "\n",
    "## 12) Quick Talking Points ğŸ¤\n",
    "\n",
    "* *â€œEach LLM/tool call is a **run**; nested runs form **traces** filterable by tags/metadata.â€*\n",
    "* *â€œWe A/B **prompts/models** on consistent datasets and **gate releases** on eval deltas.â€*\n",
    "* *â€œ**User feedback** is logged per run and tracked against **cost/latency**.â€*\n",
    "* *â€œWe export **OTel** to Prometheus/Grafana for unified SLO dashboards.â€*\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
